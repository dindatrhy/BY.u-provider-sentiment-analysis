{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter Dataset Crawling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ctRhyGY5sZcF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62599c9a-db4e-4406-ade3-a322ccfbabb6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install preprocessor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_uGpZ_3tthU",
        "outputId": "92fb80c2-c744-4924-f2f7-24862b42bde1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting preprocessor\n",
            "  Downloading preprocessor-1.1.3.tar.gz (4.2 kB)\n",
            "Building wheels for collected packages: preprocessor\n",
            "  Building wheel for preprocessor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for preprocessor: filename=preprocessor-1.1.3-py3-none-any.whl size=4477 sha256=3ff34fcb8a656cf2baa8aa9042cb67864dd79cc0f24a76729463eb657435107e\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/b7/36/aa37256db62b4bfd35a6f1b5536e9ba843f257b79dcbf3d5f1\n",
            "Successfully built preprocessor\n",
            "Installing collected packages: preprocessor\n",
            "Successfully installed preprocessor-1.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweet-preprocessor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4fqVw3fuCyH",
        "outputId": "c26086a5-5cda-4858-bcf7-6cfb900a5222"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import preprocessor as p"
      ],
      "metadata": {
        "id": "wZ3LKk2PtrWs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9JOSB84u-hU",
        "outputId": "92f760f5-f513-460d-d0a9-b9ee2b3bc839"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tweepy\n",
        "import re\n",
        "import string\n",
        "from textblob import TextBlob\n",
        "import preprocessor as p\n",
        "from preprocessor.api import clean, tokenize, parse\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import ast\n",
        "\n",
        "#the code here is based on the reference below, after some modification and combine improvement from various other sources\n",
        "#references : https://towardsdatascience.com/extracting-twitter-data-pre-processing-and-sentiment-analysis-using-python-3-0-7192bd8b47cf\n",
        "#used after some modification \n",
        "\n",
        "#Twitter credentials for the app\n",
        "consumer_key = 'WYzRl8RRj0KHBWq4GPGw17dMi'\n",
        "consumer_secret = 'DjYHqBIbcNAqf0rpslQtQ82DUvn2EOMHTkHmxTFQT88r16yD9K'\n",
        "access_key= '817260083587354624-Ks2DwMcBQQA8oP6p5bTiu5rrNrrtKxk'\n",
        "access_secret = 'WxWI1lwTJxYR3vMVdDSbC1B7M8np5UhlAgwG7i9LXdX3T'\n",
        "\n",
        "#pass twitter credentials to tweepy\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_key, access_secret)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "#file location changed to \"data/telemedicine_data_extraction/\" for clearer path\n",
        "if not os.path.exists('/content/drive/'): \n",
        "    os.mkdir('/content/drive/')\n",
        "if not os.path.exists('/content/drive/MyDrive/data_covid/data/this'):\n",
        "    os.mkdir('/content/drive/MyDrive/data_covid/data/this')\n",
        "\n",
        "byu_indonesia = \"/content/drive/MyDrive/data_covid/data/this/byu_indonesia.csv\"\n",
        "\n",
        "#columns of the csv file\n",
        "COLS = ['id', 'created_at', 'source', 'original_text','clean_text',  'lang',\n",
        "        'favorite_count', 'retweet_count', 'original_author', 'possibly_sensitive', 'hashtags',\n",
        "        'user_mentions', 'place', 'place_coord_boundaries']\n",
        "\n",
        "#set two date variables for date range\n",
        "start_date = '2021-12-01'\n",
        "end_date = '2022-01-02'\n",
        "\n",
        "# Happy Emoticons\n",
        "emoticons_happy = set([\n",
        "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
        "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
        "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
        "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
        "    '<3'\n",
        "    ])\n",
        "\n",
        "# Sad Emoticons\n",
        "emoticons_sad = set([\n",
        "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
        "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
        "    ':c', ':{', '>:\\\\', ';('\n",
        "    ])\n",
        "\n",
        "#Emoji patterns\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "\n",
        "#combine sad and happy emoticons\n",
        "emoticons = emoticons_happy.union(emoticons_sad)\n",
        "\n",
        "\n",
        "#mrhod clean_tweets()\n",
        "def clean_tweets(tweet):\n",
        "    #use slang words and stopwords to clean the data\n",
        "    # the stop words and slang words that is used, comes from https://github.com/louisowen6/NLP_bahasa_resources\n",
        "    # after some modification\n",
        "\n",
        "    my_file = open(\"/content/drive/MyDrive/data_covid/data/combined_stop_words.txt\", \"r\")\n",
        "    content = my_file.read()\n",
        "    stop_words = content.split(\"\\n\")\n",
        "    file_2  = open(\"/content/drive/MyDrive/data_covid/data/update_combined_slang_words.txt\", \"r\")\n",
        "    content2 = file_2.read()\n",
        "    slang_words = ast.literal_eval(content2)\n",
        "    my_file.close()\n",
        "    file_2.close()\n",
        "\n",
        "    tweet = tweet.lower()\n",
        "    #after tweepy preprocessing the colon left remain after removing mentions\n",
        "    #or RT sign in the beginning of the tweet\n",
        "    tweet = re.sub(r':', '', tweet)\n",
        "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
        "    #replace consecutive non-ASCII characters with a space\n",
        "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
        "\n",
        "    #remove emojis from tweet\n",
        "    tweet = emoji_pattern.sub(r'', tweet)\n",
        "\n",
        "    #remove punctuation manually\n",
        "    tweet = re.sub('[^a-zA-Z]', ' ', tweet)\n",
        "    \n",
        "    #remove tags\n",
        "    tweet=re.sub(\"&lt;/?.*?&gt;\",\"&lt;&gt;\",tweet)\n",
        "    \n",
        "    #remove digits and special chars\n",
        "    tweet=re.sub(\"(\\\\d|\\\\W)+\",\" \",tweet)\n",
        "\n",
        "    #remove other symbol from tweet\n",
        "    tweet = re.sub(r'â', '', tweet)\n",
        "    tweet = re.sub(r'€', '', tweet)\n",
        "    tweet = re.sub(r'¦', '', tweet)\n",
        "\n",
        "    #modify the slang words into a more proper one\n",
        "    word_tokens = word_tokenize(tweet)\n",
        "    for w in word_tokens:\n",
        "        if w in slang_words.keys():\n",
        "            word_tokens[word_tokens.index(w)] = slang_words[w]\n",
        "\n",
        "    #filter using NLTK library append it to a string\n",
        "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
        "    filtered_tweet = []\n",
        "\n",
        "    #looping through conditions\n",
        "    for w in word_tokens:\n",
        "        #check tokens against stop words , emoticons and punctuations\n",
        "        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
        "            filtered_tweet.append(w.lower())\n",
        "    return ' '.join(filtered_tweet)\n",
        "    #print(word_tokens)\n",
        "    #print(filtered_sentence)\n",
        "\n",
        "#method write_tweets()\n",
        "def write_tweets(keyword, file):\n",
        "    # If the file exists, then read the existing data from the CSV file.\n",
        "    if os.path.exists(file):\n",
        "        df = pd.read_csv(file, header=0)\n",
        "    else:\n",
        "        df = pd.DataFrame(columns=COLS)\n",
        "    #page attribute in tweepy.cursor and iteration\n",
        "    for page in tweepy.Cursor(api.search, q=keyword,\n",
        "                              count=200, include_rts=False, since=start_date, tweet_mode=\"extended\").pages(100):\n",
        "        for status in page:\n",
        "            new_entry = []\n",
        "            status = status._json\n",
        "\n",
        "            #when run the code, below code replaces the retweet amount and\n",
        "            #no of favorires that are changed since last download.\n",
        "            if status['created_at'] in df['created_at'].values:\n",
        "                i = df.loc[df['created_at'] == status['created_at']].index[0]\n",
        "                if status['favorite_count'] != df.at[i, 'favorite_count'] or \\\n",
        "                   status['retweet_count'] != df.at[i, 'retweet_count']:\n",
        "                    df.at[i, 'favorite_count'] = status['favorite_count']\n",
        "                    df.at[i, 'retweet_count'] = status['retweet_count']\n",
        "                continue\n",
        "\n",
        "           #tweepy preprocessing called for basic preprocessing\n",
        "            clean_text = clean(status['full_text'])\n",
        "\n",
        "            #call clean_tweet method for extra preprocessing\n",
        "                \n",
        "            filtered_tweet=clean_tweets(clean_text)\n",
        "           \n",
        "            #new entry append\n",
        "            new_entry += [status['id'], status['created_at'],\n",
        "                          status['source'], status['full_text'],filtered_tweet,  status['lang'],\n",
        "                          status['favorite_count'], status['retweet_count']]\n",
        "\n",
        "            #to append original author of the tweet\n",
        "            new_entry.append(status['user']['screen_name'])\n",
        "\n",
        "            try:\n",
        "                is_sensitive = status['possibly_sensitive']\n",
        "            except KeyError:\n",
        "                is_sensitive = None\n",
        "            new_entry.append(is_sensitive)\n",
        "\n",
        "            # hashtagas and mentiones are saved using comma separted\n",
        "            hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n",
        "            new_entry.append(hashtags)\n",
        "            mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n",
        "            new_entry.append(mentions)\n",
        "\n",
        "            #get location of the tweet if possible\n",
        "            try:\n",
        "                location = status['user']['location']\n",
        "            except TypeError:\n",
        "                location = ''\n",
        "            new_entry.append(location)\n",
        "\n",
        "            try:\n",
        "                coordinates = [coord for loc in status['place']['bounding_box']['coordinates'] for coord in loc]\n",
        "            except TypeError:\n",
        "                coordinates = None\n",
        "            new_entry.append(coordinates)\n",
        "\n",
        "            single_tweet_df = pd.DataFrame([new_entry], columns=COLS)\n",
        "            df = df.append(single_tweet_df, ignore_index=True)\n",
        "            csvFile = open(file, 'a' ,encoding='utf-8')\n",
        "    df.to_csv(csvFile, mode='a', columns=COLS, index=False, encoding=\"utf-8\")\n",
        "\n",
        "#declare keywords as a query for three categories\n",
        "byu_indonesia_keywords = 'byu_id'\n",
        "\n",
        "#call main method passing keywords and file path\n",
        "\n",
        "write_tweets(byu_indonesia_keywords,byu_indonesia)"
      ],
      "metadata": {
        "id": "M9BDxj4MMA4T"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Our dataset is collected, let's see how its structured"
      ],
      "metadata": {
        "id": "tzF3C7VXJZjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/data_covid/data/this/byu_indonesia.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "jsOV0dYSxj6D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "outputId": "c3d444e0-ab00-4dc6-8169-72f532b9cdf7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-60489799-74a9-400c-92ac-70065b3457d7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>source</th>\n",
              "      <th>original_text</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>lang</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>original_author</th>\n",
              "      <th>possibly_sensitive</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>user_mentions</th>\n",
              "      <th>place</th>\n",
              "      <th>place_coord_boundaries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1478564382783119362</td>\n",
              "      <td>Wed Jan 05 03:09:53 +0000 2022</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>@byu_id apa aku harus nunggu ber-jam² hingga a...</td>\n",
              "      <td>nunggu ber jam hingga aplikasi terbuka</td>\n",
              "      <td>in</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>rickjak16</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>byu_id</td>\n",
              "      <td>Planet Namex</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1478561754284060674</td>\n",
              "      <td>Wed Jan 05 02:59:27 +0000 2022</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>Min @byu_id tolong jawab lah. Jangan di skip.</td>\n",
              "      <td>min jawab skip</td>\n",
              "      <td>in</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Oghee_1900</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>byu_id</td>\n",
              "      <td>DKI Jakarta, Indonesia</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1478558087564836865</td>\n",
              "      <td>Wed Jan 05 02:44:53 +0000 2022</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>@byu_id min beberapa hari belakangan ini kok s...</td>\n",
              "      <td>min sinyal by tidak stabil hp nokia pluslokasi...</td>\n",
              "      <td>in</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Andi_Pi14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>byu_id</td>\n",
              "      <td>Indonesia</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1478556956310659074</td>\n",
              "      <td>Wed Jan 05 02:40:23 +0000 2022</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>ada\\n(selalu ada buat teman)\\ngokil\\n(tingkah ...</td>\n",
              "      <td>teman gila tingkah gila kumpul kepomasa sih</td>\n",
              "      <td>in</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>AlRob1179</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>byu_id</td>\n",
              "      <td>Medan Timur, Indonesia</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1478555575197732864</td>\n",
              "      <td>Wed Jan 05 02:34:54 +0000 2022</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>@SpeciaIONEe @worksfess @IndosatCare @triindon...</td>\n",
              "      <td>mengadopsi bebas aktif nomor hahaha</td>\n",
              "      <td>in</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bangvalash</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SpeciaIONEe, worksfess, IndosatCare, triindone...</td>\n",
              "      <td>Mars, PA</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60489799-74a9-400c-92ac-70065b3457d7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-60489799-74a9-400c-92ac-70065b3457d7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-60489799-74a9-400c-92ac-70065b3457d7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                    id  ... place_coord_boundaries\n",
              "0  1478564382783119362  ...                    NaN\n",
              "1  1478561754284060674  ...                    NaN\n",
              "2  1478558087564836865  ...                    NaN\n",
              "3  1478556956310659074  ...                    NaN\n",
              "4  1478555575197732864  ...                    NaN\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LhvlaOnnPdbP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}